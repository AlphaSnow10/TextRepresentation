{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPraJjNb43Sc",
        "outputId": "8662fc51-07e5-4455-b1a9-d536517e580d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi everyone welcome to session 5 in NLP', ' /n I hope you enjoying this Session', ' please give your feedback ']\n",
            "['', '/n', 'I', 'hope', 'you', 'enjoying', 'this', 'Session']\n"
          ]
        }
      ],
      "source": [
        "# 1) Sentence Segmentation\n",
        "\n",
        "text = \"Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback \"\n",
        "\n",
        "sentence = text.split(\".\")\n",
        "\n",
        "print(sentence)\n",
        "\n",
        "sentence_2_words = sentence[1].split(\" \")\n",
        "\n",
        "print(sentence_2_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Remove Punctation\n",
        "\n",
        "import string\n",
        "\n",
        "punctuation_character = string.punctuation\n",
        "\n",
        "# increase punctuation character with other\n",
        "punctuation_character += \"..\"\n",
        "\n",
        "print(punctuation_character)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO_m1e5P6Vnq",
        "outputId": "743fb228-1e6e-4b7f-cbe1-3adfc2207ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. & please give your feedback. !!! \"\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  punctuation_character = string.punctuation\n",
        "  no_punc = \"\"\n",
        "  for char in text:\n",
        "    if char not in punctuation_character:\n",
        "      no_punc = no_punc + char\n",
        "\n",
        "  return no_punc\n",
        "print((text))\n",
        "print(remove_punctuations(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f83g4JZD6s5d",
        "outputId": "c6c5e5fc-b79b-4bc3-ace2-463d665bbce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. & please give your feedback. !!! \n",
            "Hi everyone welcome to session 5 in NLP n I hope you enjoying this Session  please give your feedback  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Lower case\n",
        "\n",
        "text = \"Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback \"\n",
        "\n",
        "clean_text = text.lower()\n",
        "\n",
        "print(text)\n",
        "print(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LOqPxEY8Eph",
        "outputId": "dc58cae4-9afc-4823-d412-7b8231ee5c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback \n",
            "hi everyone welcome to session 5 in nlp. /n i hope you enjoying this session. please give your feedback \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3l-q9O481xJ",
        "outputId": "c0bc20d8-6927-418f-b6c5-30440fc500df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback \"\n",
        "\n",
        "print(text)\n",
        "print(word_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_1aTlLy8dGs",
        "outputId": "a57c8243-7d9c-4065-fc2f-c55e1747ea89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback \n",
            "['Hi', 'everyone', 'welcome', 'to', 'session', '5', 'in', 'NLP', '.', '/n', 'I', 'hope', 'you', 'enjoying', 'this', 'Session', '.', 'please', 'give', 'your', 'feedback']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(\"i\" in stop_words)\n",
        "\n",
        "stop_words.remove(\"i\")\n",
        "\n",
        "print(\"i\" in stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97cweL0k-rOU",
        "outputId": "bf2cb350-bbe6-4a3c-c8da-27a51e2e78a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Stop words\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(stop_words)\n",
        "\n",
        "text = \"Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback. !!!\"\n",
        "\n",
        "def remove_stop_words(text):\n",
        "  stop_words = stopwords.words('english')\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "  return \" \".join(filtered_text)\n",
        "\n",
        "print(text)\n",
        "print(remove_stop_words(text.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1BkyZzi9TgH",
        "outputId": "1b47d8fd-7a15-4a8d-ea16-8ddc00142f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback. !!!\n",
            "hi everyone welcome session 5 nlp . /n hope enjoying session . please give feedback . ! ! !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mishkal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aVpLONRAT4r",
        "outputId": "19c9889c-e60e-4dcf-ae9b-0ddcff684ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mishkal\n",
            "  Downloading mishkal-0.4.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting alyahmor>=0.1 (from mishkal)\n",
            "  Downloading alyahmor-0.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting arramooz-pysqlite>=0.1 (from mishkal)\n",
            "  Downloading arramooz_pysqlite-0.4.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting asmai>=0.1 (from mishkal)\n",
            "  Downloading asmai-0.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting libqutrub>=1.0 (from mishkal)\n",
            "  Downloading libqutrub-1.2.4.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting maskouk-pysqlite>=0.1 (from mishkal)\n",
            "  Downloading maskouk_pysqlite-0.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting mysam-tagmanager>=0.1 (from mishkal)\n",
            "  Downloading mysam_tagmanager-0.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting naftawayh>=0.2 (from mishkal)\n",
            "  Downloading Naftawayh-0.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting pickledb>=0.9.0 (from mishkal)\n",
            "  Downloading pickleDB-0.9.2.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyarabic>=0.6.2 (from mishkal)\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting qalsadi>=0.2 (from mishkal)\n",
            "  Downloading qalsadi-0.5-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting sylajone>=0.1 (from mishkal)\n",
            "  Downloading sylajone-0.3.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting tashaphyne>=0.3.1 (from mishkal)\n",
            "  Downloading Tashaphyne-0.3.6-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting Arabic-Stopwords>=0.4.2 (from alyahmor>=0.1->mishkal)\n",
            "  Downloading Arabic_Stopwords-0.4.3-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic>=0.6.2->mishkal) (1.17.0)\n",
            "Collecting codernitydb3 (from qalsadi>=0.2->mishkal)\n",
            "  Downloading codernitydb3-0.6.0.tar.gz (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading mishkal-0.4.1-py3-none-any.whl (789 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.5/789.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alyahmor-0.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arramooz_pysqlite-0.4.2-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asmai-0.1-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libqutrub-1.2.4.1-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading maskouk_pysqlite-0.1-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mysam_tagmanager-0.4-py3-none-any.whl (37 kB)\n",
            "Downloading Naftawayh-0.4-py3-none-any.whl (332 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.6/332.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qalsadi-0.5-py3-none-any.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.3/264.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sylajone-0.3.1-py3-none-any.whl (37 kB)\n",
            "Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Arabic_Stopwords-0.4.3-py3-none-any.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pickledb, codernitydb3\n",
            "  Building wheel for pickledb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickledb: filename=pickleDB-0.9.2-py3-none-any.whl size=4252 sha256=cd81b7b01f06bf35622a86a48e368e0cbf9089402f6a1903bfba5da943f9c183\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/99/ef/8bf37f0157e6423a373297de279351af04ad18c4136c3af121\n",
            "  Building wheel for codernitydb3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for codernitydb3: filename=codernitydb3-0.6.0-py3-none-any.whl size=59851 sha256=ef25c8c59df8edd296ea0da6d63b84b5dcd825e0fd803a60883fca4688a65868\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/96/96/58346beb99967aa576ad33aa0b75ffd3c7dfa58291bec26d7c\n",
            "Successfully built pickledb codernitydb3\n",
            "Installing collected packages: pickledb, mysam-tagmanager, pyarabic, codernitydb3, tashaphyne, maskouk-pysqlite, libqutrub, asmai, arramooz-pysqlite, Arabic-Stopwords, naftawayh, alyahmor, qalsadi, sylajone, mishkal\n",
            "Successfully installed Arabic-Stopwords-0.4.3 alyahmor-0.2 arramooz-pysqlite-0.4.2 asmai-0.1 codernitydb3-0.6.0 libqutrub-1.2.4.1 maskouk-pysqlite-0.1 mishkal-0.4.1 mysam-tagmanager-0.4 naftawayh-0.4 pickledb-0.9.2 pyarabic-0.6.15 qalsadi-0.5 sylajone-0.3.1 tashaphyne-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for arabic\n",
        "import mishkal.tashkeel\n",
        "\n",
        "vocalizer = mishkal.tashkeel.TashkeelClass()\n",
        "arabic_text = \"صباح الخير. هذه المحاضرة الخامسة \""
      ],
      "metadata": {
        "id": "7wIF0v96_WGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953c69b5-1f7e-4087-b2b8-70678a0d2ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<mishkal.tashkeel.TashkeelClass at 0x79458efdb070>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Steaming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# List words show change in steaming\n",
        "\n",
        "example_words = ['python', 'pythoneer', 'pythoning', 'pythoned', 'pythonly']\n",
        "\n",
        "for word in example_words:\n",
        "  print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFdprJsDKIpD",
        "outputId": "74bc3551-5ed0-46d9-db9a-d7f38fda6be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhp-K_ySLnMX",
        "outputId": "f72442b2-74b6-493e-feab-000d31872c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# List example of words shows effect of lemmatizer\n",
        "\n",
        "example_words = ['better']\n",
        "\n",
        "for word in example_words:\n",
        "  print(lemmatizer.lemmatize(word, pos=\"n\"))\n",
        "  print(lemmatizer.lemmatize(word, pos=\"v\"))\n",
        "  print(lemmatizer.lemmatize(word, pos=\"a\"))\n",
        "  print(lemmatizer.lemmatize(word, pos=\"r\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAuWL_AFK59_",
        "outputId": "a8aaa7be-e573-4824-925c-9b198307c531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better\n",
            "better\n",
            "good\n",
            "well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPmwZvStN8T0",
        "outputId": "48d8c421-bc4b-47bc-ba26-90280ad1fa85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) POS tags\n",
        "\n",
        "from nltk import pos_tag, pos_tag_sents\n",
        "\n",
        "text = \"Hi everyone welcome to session 5 in NLP. /n I hope you enjoying this Session. please give your feedback \"\n",
        "\n",
        "print(pos_tag(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgYWti63MLPk",
        "outputId": "c9e4c892-c284-4f3f-c918-149d76a3617f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hi', 'NNP'), ('everyone', 'NN'), ('welcome', 'NN'), ('to', 'TO'), ('session', 'NN'), ('5', 'CD'), ('in', 'IN'), ('NLP', 'NNP'), ('.', '.'), ('/n', 'NN'), ('I', 'PRP'), ('hope', 'VBP'), ('you', 'PRP'), ('enjoying', 'VBG'), ('this', 'DT'), ('Session', 'NNP'), ('.', '.'), ('please', 'VB'), ('give', 'VB'), ('your', 'PRP$'), ('feedback', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) NER Named entry recognition\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "text = \"My name is Ali, I am data scientist, I have 12$ million, I located in Egypt, I'm working at Microsoft, I borned on 19-9-2000\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzvuLg3WNqNj",
        "outputId": "93dff93c-dd64-4dc9-af24-4860a3d87298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ali PERSON\n",
            "12$ million MONEY\n",
            "Egypt GPE\n",
            "Microsoft ORG\n",
            "19-9-2000 DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v515KPycPYn0",
        "outputId": "8e7daf1e-9d1a-4f8e-eb55-08716dc3c2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWeFcBhxPqZE",
        "outputId": "caf0e4fc-0747-405b-990d-c78f3a0d66c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Q4dhk4PrZV",
        "outputId": "b58a0172-0e90-4681-98a1-0a60d6c9db57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Chunking\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "text = \"I will go to the coffee shop \"\n",
        "ne_chunk(pos_tag(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "LKql-jlROzBY",
        "outputId": "bc98ee8f-fb61-43fd-90f4-2daec358b105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('I', 'PRP'), ('will', 'MD'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), ('coffee', 'NN'), ('shop', 'NN')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,304.0,120.0\" width=\"304px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"13.1579%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.57895%\" y1=\"20px\" y2=\"48px\" /><svg width=\"15.7895%\" x=\"13.1579%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.0526%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.5263%\" x=\"28.9474%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">go</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.2105%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.5263%\" x=\"39.4737%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.7368%\" y1=\"20px\" y2=\"48px\" /><svg width=\"13.1579%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.5789%\" y1=\"20px\" y2=\"48px\" /><svg width=\"21.0526%\" x=\"63.1579%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">coffee</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.6842%\" y1=\"20px\" y2=\"48px\" /><svg width=\"15.7895%\" x=\"84.2105%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">shop</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"92.1053%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}